{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "low_memory=False\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction & Problem Setting\n",
    "\n",
    "As said during the first lesson, the most important purpose of data processing is making sure the data we provide contains **no garbage**. Because of this, we have to explore our dataset, figure out exactly how its constructed and which values it contains so we can fix the 'cracks'. Only then will we be able to end up with a qualitative dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Explore the content of the dataset\n",
    "\n",
    "Let's dive into it! Today we will be working with the titanic dataset, it is a common dataset which contains data about the passengers on the titanic at the time it sank. Load in the dataset and have a quick look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic_uncleaned.csv\", sep = \";\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at some data is by using .sample instead of .head. Here you can pass along a number to determine your amount of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Based on the output above, what is the main difference between .head and .sample? And why would you use .sample over .head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.get_dummies(titanic, columns = ['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also seen another method to explore our dataset, this was by looking at the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Looking at the datatypes of each column, which ones will we likely need to adapt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to explore our dataset is by using the describe function. This function gives you a lot of information more on the statistical side of things, such as the minimun value, maximum value, average value, and much more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we would create Series filled with booleans when selecting subsets of the data, we can create entire dataframes of booleans! The most useful case for this is when identifyin missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what are we with this information? Well simple! When combining it with our earlier seen .describe method, we get some slightly different behaviour giving us a great overview of the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Try and interpret the graph above. Where do the most missing values lie? Are there collumns with no missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 First data transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've talked about modifying data types, now let's actually start doing this!\n",
    "\n",
    "The easiest and most common way to do so is called **one-hot encoding**, also called **dummy variables**. The theory behind this is simple: when a variable is binary, meaning either A or B, we can represent this wit hthe numbers 0 and 1.\n",
    "\n",
    "This column then represents **whether the given variable is present**. This is important, because it means we must pick what are interpretation of 'the variable being present' is.\n",
    "\n",
    "When looking at the titanic dataset, a first column we may want to encode this way is 'Survived'. In this case we would define a passenger surviving as the variable being present. This means that 'Yes' becomes '1' and 'No' becomes '0'.\n",
    "\n",
    "Now the question remains: how do we actually implement this? Do we go over each record and manually replace the values? Of course not, that would take waaay too much time. By now you should know that for everything you can think of, someone probably already wrote a python package. Luckily for us, this functionality is baked right into pandas with the .map function.\n",
    "\n",
    "All we have to do is call the column we want to map and pass on a dictionary of our mapping where the keys are the variables we want to replace and the values are the variables we wish to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Survived'] = titanic['Survived'].map({'No': 0, 'Yes' : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: One-hot encoding works by indicating whether a certain variable is present, thus limiting it to binary columns only. This means we cannot simply apply it to columns like 'Embarked'. Can you think of a way to use this dummy variables technique anyways to encode such columns? Are there any new limitations for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now, are these the only ways to execute this transformation? Of course not! Often you will find datasets already use booleans to indicate binary data. Well, pandas makes it super easy to 'swap' datatypes, on the condition that they are compatible (such as True/False & 1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Embarked_C'] = titanic['Embarked_C'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever lose track of what datatype you are currently working with, remember to simply check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Survived'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Missing values\n",
    "\n",
    "There are three main ways to handle missing data:\n",
    "\n",
    "- Removing data\n",
    "- Data imputation\n",
    "- Data flagging\n",
    "\n",
    "Removing data is the simplest method of them all, but also leads to the most information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2 = titanic.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how easy it was to drop all the missing data, but how in the end it led to us retaining only 185 rows of the 891 rows? That is a huge loss! This is because by default the .dropna() function will delete any row where a missing value is present. to tweak this a bit, we can adjust the treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2 = titanic.dropna(thresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.isna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed no missing values... You can see how this threshold is a finnicky thing to play with!\n",
    "\n",
    "Another way to handle these empty values is by filling them in with a 'good enough' value. There are several default functions to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = titanic['Age'].ffill()\n",
    "age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = titanic['Age'].bfill()\n",
    "age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a default function isn't good enough or makes no sense for the use case, and then you have to code a custom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = titanic['Age'].fillna(titanic[titanic['Age'].notna()].Age.median())\n",
    "age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to be extremely careful when filling the data like this! Its easy for a value to look good, but completely ruin the dataset! That's why sometimes it is better to simply 'flag' the missing values. This way they get an assigned value, but this assigned value won't (if you did it correctly) ruin your dataset. \n",
    "\n",
    "Again, it's important to make sure you pick values that **do not ruin your dataset**! Don't pick extremely low or high values for example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = titanic['Age'].fillna(0)\n",
    "age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
